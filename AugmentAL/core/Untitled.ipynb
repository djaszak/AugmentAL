{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3843995-ba82-4c32-b703-1c501e48f5d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'core.constants'; 'core' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloop\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_active_learning_loop\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Datasets, TransformerModels\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n",
      "File \u001b[0;32m/home/h5/s8822750/AugmentAL/AugmentAL/core/core.py:17\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmall_text\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     PoolBasedActiveLearner,\n\u001b[1;32m      9\u001b[0m     PredictionEntropy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     Classifier,\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerModels\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# CONSTANTS\u001b[39;00m\n\u001b[1;32m     21\u001b[0m SEED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2022\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'core.constants'; 'core' is not a package"
     ]
    }
   ],
   "source": [
    "from core.loop import run_active_learning_loop\n",
    "from core.constants import Datasets, TransformerModels\n",
    "from datasets import load_dataset\n",
    "from nlpaug.augmenter import word as naw\n",
    "from core.augment import create_augmented_dataset\n",
    "\n",
    "query_strategies = [\n",
    "    # \"BreakingTies\",\n",
    "    # \"AugmentedSearchSpaceExtensionAndOutcomeQueryStrategy\",\n",
    "    \"AugmentedSearchSpaceExtensionQueryStrategy\",\n",
    "    # \"AugmentedOutcomesQueryStrategy\",\n",
    "    # \"AugmentedSearchSpaceExtensionAndOutcomeLeastConfidenceQueryStrategy\",\n",
    "    # \"AugmentedSearchSpaceExtensionLeastConfidenceQueryStrategy\",\n",
    "    # \"AugmentedOutcomesLeastConfidenceQueryStrategy\",\n",
    "]\n",
    "\n",
    "num_queries = 20\n",
    "num_samples = 20\n",
    "num_augmentations = 5\n",
    "\n",
    "datasets = [Datasets.ROTTEN.value]\n",
    "\n",
    "for dataset_name in datasets:\n",
    "\n",
    "    dataset = load_dataset(dataset_name)\n",
    "\n",
    "    raw_test = dataset[\"test\"]\n",
    "    raw_train = dataset[\"train\"]\n",
    "    raw_augmented_train, augmented_indices = create_augmented_dataset(\n",
    "        raw_train, naw.SynonymAug(aug_src=\"wordnet\"), n=num_augmentations\n",
    "    )\n",
    "\n",
    "    for query_strategy in query_strategies:\n",
    "        run_active_learning_loop(\n",
    "            raw_test,\n",
    "            raw_train,\n",
    "            raw_augmented_train,\n",
    "            augmented_indices,\n",
    "            num_queries=num_queries,\n",
    "            num_samples=num_samples,\n",
    "            num_augmentations=num_augmentations\n",
    "            if query_strategy != \"BreakingTies\"\n",
    "            else 0,\n",
    "            query_strategy=query_strategy,\n",
    "            model=TransformerModels.BERT.value,\n",
    "            device=\"cuda\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d89f75-a70e-4f5a-b719-ae2288953c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/h5/s8822750/AugmentAL/AugmentAL/core'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790b9d46-ffbb-4549-b1eb-2c0225c85ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/h5/s8822750/AugmentAL/AugmentAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/util/JupyterLab/alpha/share/pytorch_v2/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591fe688-5bfa-4461-aae3-15aa447f5702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d236e4f2ce7c4182958269a0466342bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cd223fb9c1438db7f8aa816e539a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60aece94135c49db8512a85fb448168b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98629ed5fad0483f97536712984642ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e5f40e233f466d94c2c60e8ffbdb8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from core.loop import run_active_learning_loop\n",
    "from core.constants import Datasets, TransformerModels\n",
    "from datasets import load_dataset\n",
    "from nlpaug.augmenter import word as naw\n",
    "from core.augment import create_augmented_dataset\n",
    "\n",
    "query_strategies = [\n",
    "    # \"BreakingTies\",\n",
    "    # \"AugmentedSearchSpaceExtensionAndOutcomeQueryStrategy\",\n",
    "    \"AugmentedSearchSpaceExtensionQueryStrategy\",\n",
    "    # \"AugmentedOutcomesQueryStrategy\",\n",
    "    # \"AugmentedSearchSpaceExtensionAndOutcomeLeastConfidenceQueryStrategy\",\n",
    "    # \"AugmentedSearchSpaceExtensionLeastConfidenceQueryStrategy\",\n",
    "    # \"AugmentedOutcomesLeastConfidenceQueryStrategy\",\n",
    "]\n",
    "\n",
    "num_queries = 20\n",
    "num_samples = 20\n",
    "num_augmentations = 5\n",
    "\n",
    "datasets = [Datasets.ROTTEN.value]\n",
    "\n",
    "for dataset_name in datasets:\n",
    "\n",
    "    dataset = load_dataset(dataset_name)\n",
    "\n",
    "    raw_test = dataset[\"test\"]\n",
    "    raw_train = dataset[\"train\"]\n",
    "    raw_augmented_train, augmented_indices = create_augmented_dataset(\n",
    "        raw_train, naw.SynonymAug(aug_src=\"wordnet\"), n=num_augmentations\n",
    "    )\n",
    "\n",
    "    for query_strategy in query_strategies:\n",
    "        run_active_learning_loop(\n",
    "            raw_test,\n",
    "            raw_train,\n",
    "            raw_augmented_train,\n",
    "            augmented_indices,\n",
    "            num_queries=num_queries,\n",
    "            num_samples=num_samples,\n",
    "            num_augmentations=num_augmentations\n",
    "            if query_strategy != \"BreakingTies\"\n",
    "            else 0,\n",
    "            query_strategy=query_strategy,\n",
    "            model=TransformerModels.BERT.value,\n",
    "            device=\"cuda\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe66032-818a-4298-8bc7-22a40991d1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch v2.x (Machine Learning)",
   "language": "python",
   "name": "pytorch_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
